{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence of Feature Selection and PCA on a Small Dataset\n",
    "This notebook covers the influence of feature selection and PCA on the Titanic Survivors dataset. Most of the preprocessing code such as data cleaning, encoding and transformation is adapted from the [Scikit-Learn ML from Start to Finish](https://www.kaggle.com/jeffd23/scikit-learn-ml-from-start-to-finish) work by [Jeff Delaney](https://www.kaggle.com/jeffd23).\n",
    "\n",
    "## Import Data\n",
    "Load the csv train and test files into a pandas dataframe and print the first 5 rows to see a sample of the data. Print also a statistics description of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise Data\n",
    "To familiarise with the data and discover underlying patterns to exploit later in the machine learning models, we need to create some distribution, bar, and scatter plots. For a complete visualisation analysis check [Scikit-Learn ML from Start to Finish](https://www.kaggle.com/jeffd23/scikit-learn-ml-from-start-to-finish) work and my previous work [here](https://github.com/gtraskas/Udacity/tree/master/Data%20Analyst%20Nanodegree/P3_Intro_to_Data_Analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineer Features\n",
    "1. Aside from 'Sex', the 'Age' feature is second in importance. To avoid overfitting, group people into logical human age groups.\n",
    "2. Each 'Cabin' starts with a letter. Probably, this letter is more important than the number that follows, so slice it off.\n",
    "3. 'Fare' is another continuous value that should be simplified, placing the values into quartile bins accordingly.\n",
    "4. Extract information from the 'Name' feature. Rather than use the full name, extract the last name and prefix and then append them as their own features.\n",
    "5. Lastly, drop useless features ('Ticket', 'Name', and 'Embarked')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from https://www.kaggle.com/jeffd23/scikit-learn-ml-from-start-to-finish\n",
    "def simplify_ages(df):\n",
    "    df.Age = df.Age.fillna(-0.5)\n",
    "    bins = (-1, 0, 5, 12, 18, 25, 35, 60, 120)\n",
    "    group_names = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\n",
    "    categories = pd.cut(df.Age, bins, labels=group_names)\n",
    "    df.Age = categories\n",
    "    return df\n",
    "\n",
    "def simplify_cabins(df):\n",
    "    df.Cabin = df.Cabin.fillna('N')\n",
    "    df.Cabin = df.Cabin.apply(lambda x: x[0])\n",
    "    return df\n",
    "\n",
    "def simplify_fares(df):\n",
    "    df.Fare = df.Fare.fillna(-0.5)\n",
    "    bins = (-1, 0, 8, 15, 31, 1000)\n",
    "    group_names = ['Unknown', '1_quartile', '2_quartile', '3_quartile', '4_quartile']\n",
    "    categories = pd.cut(df.Fare, bins, labels=group_names)\n",
    "    df.Fare = categories\n",
    "    return df\n",
    "\n",
    "def format_name(df):\n",
    "    df['Lname'] = df.Name.apply(lambda x: x.split(' ')[0])\n",
    "    df['NamePrefix'] = df.Name.apply(lambda x: x.split(' ')[1])\n",
    "    return df    \n",
    "    \n",
    "def drop_features(df):\n",
    "    return df.drop(['Ticket', 'Name', 'Embarked'], axis=1)\n",
    "\n",
    "def transform_features(df):\n",
    "    df = simplify_ages(df)\n",
    "    df = simplify_cabins(df)\n",
    "    df = simplify_fares(df)\n",
    "    df = format_name(df)\n",
    "    df = drop_features(df)\n",
    "    return df\n",
    "\n",
    "train_df = transform_features(train_df)\n",
    "test_df = transform_features(test_df)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Data\n",
    "Normalize and transform categorical non-numerical features to numerical with the `LabelEncoder` tool from `scikit-learn`, making out data more flexible for various algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def encode_features(df_train, df_test):\n",
    "    features = ['Fare', 'Cabin', 'Age', 'Sex', 'Lname', 'NamePrefix']\n",
    "    df_combined = pd.concat([df_train[features], df_test[features]])\n",
    "    \n",
    "    for feature in features:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le = le.fit(df_combined[feature])\n",
    "        df_train[feature] = le.transform(df_train[feature])\n",
    "        df_test[feature] = le.transform(df_test[feature])\n",
    "    return df_train, df_test\n",
    "    \n",
    "train_df, test_df = encode_features(train_df, test_df)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "### Split Data to Train/Test Sets\n",
    "Create train/test sets using the `train_test_split` function. The `test_size=0.2` indicates the percentage of the data that should be held over for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the independent variables as features.\n",
    "Xs = train_df.drop(['PassengerId', 'Survived'], axis=1)\n",
    "\n",
    "# Define the target (dependent) variable as labels.\n",
    "Ys = train_df['Survived']\n",
    "\n",
    "# Create a train/test split using 30% test size.\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs,\n",
    "                                                    Ys,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=23)\n",
    "\n",
    "# Check the split printing the shape of each set.\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Algorithms\n",
    "Test and evaluate 5 algorithms:\n",
    "1. Naive Bayes\n",
    "2. Support Vector Machines\n",
    "3. K Nearest Neighbors\n",
    "4. Random Forest\n",
    "5. AdaBoost\n",
    "\n",
    "### Validate\n",
    "\n",
    "Measure the effectiveness of the algorithm applying `KFold`. Split the data into 50 buckets, then run the algorithm using a different bucket as the test set for each iteration. Turn `shuffle=True` to shuffle the data points' order before splitting into folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from time import time\n",
    "\n",
    "# Create the classifiers.\n",
    "nb = GaussianNB()\n",
    "svc = SVC()\n",
    "dtc = DecisionTreeClassifier()\n",
    "knc = KNeighborsClassifier()\n",
    "rfc = RandomForestClassifier()\n",
    "abc = AdaBoostClassifier()\n",
    "\n",
    "# Create a dictionary of classifiers to choose from.\n",
    "classifiers = {\"GaussianNB\": nb, \"SVM\": svc, \"Decision Trees\": dtc, \n",
    "               \"KNN\": knc, \"Random Forest\": rfc, \"AdaBoost\": abc}\n",
    "\n",
    "# Create a function that runs and evaluates the classifiers.\n",
    "def test_clfs(clf):\n",
    "    \n",
    "    # Create the KFold cross validation iterator.\n",
    "    kf = KFold(n_splits=50, shuffle=True, random_state=23)\n",
    "    \n",
    "    outcomes = []\n",
    "    fold = 0\n",
    "    for train_index, test_index in kf.split(Xs):\n",
    "        t0 = time()\n",
    "        fold += 1\n",
    "        X_train, X_test = Xs.values[train_index], Xs.values[test_index]\n",
    "        y_train, y_test = Ys.values[train_index], Ys.values[test_index]\n",
    "           \n",
    "        # Fit the classifier to the data.\n",
    "        clf.fit(X_train, y_train)\n",
    "            \n",
    "        # Create a set of predictions.\n",
    "        predictions = clf.predict(X_test)\n",
    "        # Evaluate predictions with accuracy score.\n",
    "        accuracy = clf.score(X_test, y_test)\n",
    "            \n",
    "        outcomes.append(accuracy)\n",
    "            \n",
    "    mean_outcome = np.mean(outcomes)\n",
    "        \n",
    "    # Print the results.\n",
    "    print(\"\\nMean Accuracy: {0}\".format(mean_outcome))\n",
    "    print(\"\\nTime passed: \", round(time() - t0, 3), \"s\\n\")\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(\"#\"*55)\n",
    "    print(name)\n",
    "    test_clfs(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Select **Adaboost** as the best algorithm, since it gives the best accuracy scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence of Feature Selection & PCA\n",
    "\n",
    "Investigate if Feature Selection and `PCA` can improve the performance of AdaBoost.\n",
    "\n",
    "#### Feature Selection\n",
    "\n",
    "Select the best features with `SelectKBest`, which removes all but the k highest scoring features.\n",
    "\n",
    "#### PCA\n",
    "\n",
    "Reduce the dimensionnality of the data using the Principal Component Analysis (`PCA`).\n",
    "\n",
    "#### Dimensionality Reduction\n",
    "\n",
    "Use `GridSearchCV` and `Pipeline` to optimize over different classes of estimators. Compare unsupervised `PCA` dimensionality reduction to univariate feature selection `SelectKBest` during the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer, accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Create the classifier.\n",
    "clf = AdaBoostClassifier()\n",
    "\n",
    "# Create the pipeline.\n",
    "pipeline = Pipeline([('reduce_dim', PCA()),\n",
    "                     ('clf', clf)])\n",
    "\n",
    "# Create the parameters.\n",
    "n_feature_options = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "n_estimators = [50]\n",
    "parameters = [{'reduce_dim': [PCA(iterated_power=7)],\n",
    "               'reduce_dim__n_components': n_feature_options,\n",
    "               'clf__n_estimators': n_estimators},\n",
    "              {'reduce_dim': [SelectKBest()],\n",
    "               'reduce_dim__k': n_feature_options,\n",
    "               'clf__n_estimators': n_estimators}]\n",
    "\n",
    "reducer_labels = ['PCA', 'KBest()']\n",
    "\n",
    "# Create a function to get the best estimator and print the reports.\n",
    "def compare_estimators():\n",
    "    t0 = time()\n",
    "\n",
    "    # Create the KFold cross-validator.\n",
    "    kf = KFold(n_splits=50, shuffle=True, random_state=23)\n",
    "\n",
    "    # Create accuracy score to compare each combination.\n",
    "    scoring = {'Accuracy': make_scorer(accuracy_score)}\n",
    "\n",
    "    # Create the grid search.\n",
    "    grid = GridSearchCV(estimator=pipeline,\n",
    "                        param_grid=parameters,\n",
    "                        scoring=scoring,\n",
    "                        cv=kf, refit='Accuracy')\n",
    "\n",
    "    # Fit grid search combinations.\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions.\n",
    "    predictions = grid.predict(X_test)\n",
    "\n",
    "    # Evaluate using sklearn.classification_report().\n",
    "    report = classification_report(y_test, predictions)\n",
    "\n",
    "    # Get the best parameters and scores.\n",
    "    best_parameters = grid.best_params_\n",
    "    best_score = grid.best_score_\n",
    "    \n",
    "    mean_scores = np.array(grid.cv_results_['mean_test_Accuracy'])\n",
    "    # scores are in the order of param_grid iteration, which is alphabetical\n",
    "    mean_scores = mean_scores.reshape(len(n_estimators), -1, len(n_feature_options))\n",
    "    # select score for best C\n",
    "    mean_scores = mean_scores.max(axis=0)\n",
    "    bar_offsets = (np.arange(len(n_feature_options)) *\n",
    "                   (len(reducer_labels) + 1) + .5)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for i, (label, reducer_scores) in enumerate(zip(reducer_labels, mean_scores)):\n",
    "        plt.bar(bar_offsets + i, reducer_scores, label=label)\n",
    "\n",
    "    plt.title(\"Comparing feature reduction techniques\")\n",
    "    plt.xlabel('Reduced number of features')\n",
    "    plt.xticks(bar_offsets + len(reducer_labels) / 2, n_feature_options)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim((0, 1))\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    # Print the results.\n",
    "    print(\"\\nAccuracy score: \", accuracy_score(y_test, predictions))\n",
    "    print(\"\\nReport:\\n\")\n",
    "    print(report)\n",
    "    print(\"\\nBest Mean Accuracy score: \", best_score)\n",
    "    print(\"\\nBest parameters:\\n\")\n",
    "    print(best_parameters)\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "    print(\"Time passed: \", round(time() - t0, 3), \"s\")\n",
    "    \n",
    "    return grid.best_estimator_\n",
    "\n",
    "compare_estimators()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It seems that `SelectKBest` performs better than `PCA` for all the cases, but the accuracy score has not improved significantly after dimensionality reduction. Actually, it was found that the best model keeps all the features (k=9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune\n",
    "\n",
    "To get an improved performance, optimise the hyperparameters that impact the model using `GridSearchCV`. The effectiveness of the algorithm is validated with `StratifiedShuffleSplit` and the evaluation with multiple metrics such as accuracy, precision, and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Create the classifier.\n",
    "clf = AdaBoostClassifier()\n",
    "\n",
    "# Create the parameters.\n",
    "parameters = {'n_estimators': [10, 25, 50, 75],\n",
    "              'algorithm': ['SAMME', 'SAMME.R'],\n",
    "              'random_state': [3]}\n",
    "\n",
    "# Find the best estimator and print the reports.\n",
    "t0 = time()\n",
    "\n",
    "# Create the Stratified ShuffleSplit cross-validator.\n",
    "sss = StratifiedShuffleSplit(n_splits=50, test_size=0.2, random_state=3)\n",
    "\n",
    "# Create multiple evaluation metrics to compare each combination.\n",
    "scoring = {'AUC': 'roc_auc',\n",
    "           'Accuracy': make_scorer(accuracy_score),\n",
    "           'Precision': 'precision',\n",
    "           'Recall': 'recall',\n",
    "           'f1': 'f1'}\n",
    "\n",
    "# Create the grid search.\n",
    "grid = GridSearchCV(estimator=clf,\n",
    "                    param_grid=parameters,\n",
    "                    scoring=scoring,\n",
    "                    cv=sss, refit='Accuracy')\n",
    "\n",
    "# Fit grid search combinations.\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = grid.predict(X_test)\n",
    "\n",
    "# Evaluate using sklearn.classification_report().\n",
    "report = classification_report(y_test, predictions)\n",
    "\n",
    "# Get the best parameters and scores.\n",
    "best_parameters = grid.best_params_\n",
    "best_score = grid.best_score_\n",
    "\n",
    "# Print the results.\n",
    "print(\"\\nAccuracy score: \", accuracy_score(y_test, predictions))\n",
    "print(\"\\nReport:\\n\")\n",
    "print(report)\n",
    "print(\"\\nBest Accuracy score: \", best_score)\n",
    "print(\"\\nBest parameters:\\n\")\n",
    "print(best_parameters)\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(\"Time passed: \", round(time() - t0, 3), \"s\")\n",
    "\n",
    "best_clf = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the Actual Test Data\n",
    "Finally, make the predictions and export them to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passenger_ids = test_df['PassengerId']\n",
    "predictions = best_clf.predict(test_df.drop('PassengerId', axis=1))\n",
    "\n",
    "output = pd.DataFrame({ 'PassengerId' : passenger_ids, 'Survived': predictions })\n",
    "output.to_csv('titanic_predictions.csv', index = False)\n",
    "output.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
